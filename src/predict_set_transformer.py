"""
ä½¿ç”¨è®­ç»ƒå¥½çš„Set Transformeræ¨¡åž‹å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ã€‚

è¾“å…¥: filtered_test_dataset_1000.csv (1000ä¸ªæµ‹è¯•åˆ†å­)
è¾“å‡º: test_predictions_1000.csv (Kaggleæäº¤æ ¼å¼)
"""
import torch
import numpy as np
import pandas as pd
from tqdm import tqdm
from set_transformer_model import create_model
from set_transformer_data import MolecularGraphDataset, collate_fn
from torch.utils.data import DataLoader
import argparse
import os


def predict_test_set(
    model_path,
    test_csv,
    output_csv,
    batch_size=8,
    device='cpu'
):
    """
    å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ã€‚
    
    Args:
        model_path: è®­ç»ƒå¥½çš„æ¨¡åž‹checkpointè·¯å¾„
        test_csv: æµ‹è¯•é›†CSVæ–‡ä»¶ (åŒ…å«smilesåˆ—)
        output_csv: è¾“å‡ºé¢„æµ‹ç»“æžœCSV
        batch_size: æ‰¹å¤§å°
        device: è®¾å¤‡ (cpu/cuda)
    """
    print("="*60)
    print("Set Transformer Test Set Prediction")
    print("="*60)
    
    # Load model
    print(f"\nLoading model from {model_path}...")
    checkpoint = torch.load(model_path, map_location=device)
    config = checkpoint['config']
    
    model = create_model(config['model'])
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()
    
    print(f"Loaded model from epoch {checkpoint['epoch']}")
    print(f"Best validation loss: {checkpoint.get('best_val_loss', 'N/A')}")
    
    # Load test data
    print(f"\nLoading test data from {test_csv}...")
    df_test = pd.read_csv(test_csv)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰molecule_nameåˆ—
    if 'molecule_name' not in df_test.columns:
        print("Warning: No molecule_name column, using index as molecule_name")
        df_test['molecule_name'] = [f'mol_{i}' for i in range(len(df_test))]
    
    smiles_list = df_test['smiles'].tolist()
    molecule_names = df_test['molecule_name'].tolist()
    
    print(f"Test set size: {len(smiles_list)} molecules")
    
    # Create dataset (without target couplings for test set)
    # ä½¿ç”¨ç©ºåˆ—è¡¨ä½œä¸ºå ä½ç¬¦
    empty_couplings = [[] for _ in range(len(smiles_list))]
    
    dataset = MolecularGraphDataset(
        smiles_list,
        empty_couplings,
        max_atoms=config['model']['max_atoms']
    )
    
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        collate_fn=collate_fn,
        shuffle=False
    )
    
    # Predict
    print("\nPredicting...")
    all_predictions = []
    
    # ç±»åž‹ç¼–ç æ˜ å°„ - å¿…é¡»ä¸Žè®­ç»ƒæ—¶çš„MolecularGraphDatasetä¸€è‡´ï¼
    type_names = ['1JHC', '2JHC', '3JHC', '1JHH', '2JHH', '3JHH', '2JHN', '3JHN']
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(dataloader)):
            # Move to device
            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v 
                    for k, v in batch.items()}
            
            # Forward pass
            output = model(batch)
            
            # Get predictions
            pred_j = output['j_values']  # [batch, max_atoms^2]
            pred_type_logits = output['type_logits']  # [batch, max_atoms^2, n_types]
            pred_types = torch.argmax(pred_type_logits, dim=-1)  # [batch, max_atoms^2]
            pred_mask = batch['pred_mask']  # [batch, max_atoms^2]
            
            # Extract atom types
            atom_types_batch = batch['atom_types']  # [batch, max_atoms]
            
            # Process each molecule in batch
            batch_size_actual = len(pred_j)
            for i in range(batch_size_actual):
                mol_idx = batch_idx * batch_size + i
                if mol_idx >= len(molecule_names):
                    break
                
                mol_name = molecule_names[mol_idx]
                smiles = smiles_list[mol_idx]
                
                # Get valid atoms (non-padding)
                atom_types = atom_types_batch[i].cpu().numpy()
                valid_atoms = atom_types > 0
                n_atoms = valid_atoms.sum()
                
                # Get valid predictions
                valid_pred_idx = pred_mask[i].cpu().numpy()
                
                # Iterate through all atom pairs
                max_atoms = config['model']['max_atoms']
                for pair_flat_idx in range(len(valid_pred_idx)):
                    if not valid_pred_idx[pair_flat_idx]:
                        continue
                    
                    # Convert flat index to (atom_i, atom_j)
                    # æ¨¡åž‹è¾“å‡ºæ˜¯max_atoms x max_atomsçš„å±•å¹³
                    atom_i = pair_flat_idx // max_atoms
                    atom_j = pair_flat_idx % max_atoms
                    
                    # è·³è¿‡paddingåŽŸå­
                    if atom_i >= n_atoms or atom_j >= n_atoms:
                        continue
                    
                    # è·³è¿‡è‡ªçŽ¯
                    if atom_i == atom_j:
                        continue
                    
                    j_value = float(pred_j[i, pair_flat_idx].cpu().item())
                    coupling_type = int(pred_types[i, pair_flat_idx].cpu().item())
                    
                    # è¿‡æ»¤å¼‚å¸¸å€¼
                    if j_value < 0 or j_value > 300:  # Jå€¼é€šå¸¸åœ¨0-300 HzèŒƒå›´
                        continue
                    
                    type_str = type_names[coupling_type] if coupling_type < len(type_names) else 'unknown'
                    
                    all_predictions.append({
                        'molecule_name': mol_name,
                        'atom_index_0': int(atom_i),
                        'atom_index_1': int(atom_j),
                        'scalar_coupling_constant': j_value,
                        'type': type_str
                    })
    
    # Create output DataFrame
    df_pred = pd.DataFrame(all_predictions)
    
    print(f"\nâœ… Generated {len(df_pred)} predictions")
    print(f"   From {len(set(df_pred['molecule_name']))} molecules")
    
    # Statistics
    print("\nPrediction statistics:")
    print(f"  J-coupling range: [{df_pred['scalar_coupling_constant'].min():.2f}, {df_pred['scalar_coupling_constant'].max():.2f}] Hz")
    print(f"  J-coupling mean: {df_pred['scalar_coupling_constant'].mean():.2f} Hz")
    print(f"  J-coupling median: {df_pred['scalar_coupling_constant'].median():.2f} Hz")
    
    print("\nType distribution:")
    type_counts = df_pred['type'].value_counts()
    for t, count in type_counts.items():
        print(f"  {t}: {count} ({count/len(df_pred)*100:.1f}%)")
    
    # Save
    df_pred.to_csv(output_csv, index=False)
    print(f"\nðŸ’¾ Predictions saved to {output_csv}")
    
    return df_pred


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--checkpoint', type=str, required=True,
                       help='Path to trained model checkpoint')
    parser.add_argument('--test', type=str, default='filtered_test_dataset_1000.csv',
                       help='Test dataset CSV file')
    parser.add_argument('--output', type=str, default='test_predictions_1000.csv',
                       help='Output predictions CSV file')
    parser.add_argument('--batch_size', type=int, default=8,
                       help='Batch size for prediction')
    args = parser.parse_args()
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    predictions = predict_test_set(
        args.checkpoint,
        args.test,
        args.output,
        batch_size=args.batch_size,
        device=device
    )
    
    print("\n" + "="*60)
    print("Prediction completed!")
    print("="*60)


if __name__ == '__main__':
    main()
